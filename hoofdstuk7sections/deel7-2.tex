\subsection{Opgeslagen video streamen}

\noindent Streaming video systemen kunnen geclassificeerd worden in drie categorieën: UDP streaming, HTTP streaming en adaptive HTTP streaming. Hoewel al e drie types in de praktijk gebruikt worden, gebruikt de meerderheid van de huidige systemen HTTP streaming en adaptive HTTP streaming.
Een gemeenschappelijk kenmerk van alle drie vormen is het veelvuldig gebruik van client side applicatie buffering om de effecten van verschillend end-to-end vertragingen en verschillende hoeveelheden van beschikbare bandbreedte tussen server en client te verzachten.

\noindent Algemeen kunnen gebruikers een lichte vertraging tolereren tussen de aanvraag van de video en het wanneer de video begint af te spelen. Dus wanneer de video begint aan te komen bij de client, moet de client de video niet direct af te spelen, maar kan het ondertussen een reserve van een aantal seconden opbouwen van gebufferde maar nog niet afgespeelde video, de client kan dan beginnen met het afspelen van de video. Er zijn twee belangrijke voordelen die voorzien worden bij client buffering. De client kan ten eerste variaties in server naar client vertraging absorberen. Als een bepaald stuk vertraagd is, kan het geen kwaad zolang het aankomt voordat de reserve leeg is. Ten tweede als de server naar client bandbreedte plotseling onder de video consumptie ratio zakt, kan de client verder met doorlopende afspeling, zolang als de client applicatie buffer niet volledig geledigd is.

\subsubsection{UDP streaming}

\noindent Bij UDP streaming, verzend de server de video op een snelheid die overeenkomt met de consumptie ratio van de client door de video stukken over UDP in de ready state uit te klokken. UDP gebruikt geen congestie-controle mechanisme, de server kan dus de pakketten door het netwerken duwen aan de consumptie ratio van de video. UDP streaming gebruikt een kleine client-side buffer die groot genoeg is om minder dan een seconde van een video te houden. Voor de video stukjes aan UDP te geven, gaat de server de video stukjes encapsuleren binnen transport pakketten die speciaal ontworpen zijn om audio en video te transporteren, door gebruik te maken van het Real-Time Transport Protocol (RTP). Nog een andere onderscheidende kenmerk van UDP streaming is dat in toevoeging aan het server-to-client video stream, de client en server ook een apparte parallelle controle connectie onderhouden op welke de client commando’s zend over de sessie status veranderingen. Deze controle connectie is analoog met het FTP controle verbinding.
Hoewel UDP streaming in veel open source systemen en eigen producten gebruikt wordt, leidt het aan drie significante nadelen. Ten eerste, door de onvoorspelbaarheid en wisselende aantal van beschikbare bandbreedte tussen de server en client, kan constante snelheid falen om doorlopende afspeling te voorzien. Het tweede nadeel is dat het een media control server vereist, zoals een RTSP server, om client-to-server interactie verzoeken te behandelen en om de client status bij te houden van elke lopende client sessie. De derde nadeel is dat veel firewalls zo geconfigureerd zijn om UDP verkeer te blokkeren, wat gebruikers achter de firewall verhinderd om UDP video te krijgen.

\subsubsection{HTTP streaming}

\noindent Bij HTTP streaming, is de video opgeslagen in een HTTP server als een gewoon bestand met een specifieke URL. Wanneer een gebruiker een video wilt zien, zet de client een TCP connectie op met de server en vraagt een HTTP GET request voor dat URL. De server zend dan zo snel mogelijk, als de TCP congestie controle en stroom, de video binnen een HTTP response bericht toe laat. Aan de client-side worden de bytes verzameld in de applicatie buffer. Wanneer de aantal bytes in deze buffer een bepaald aantal overschrijdt, gaat de applicatie beginnen met het afspelen. (Het pakt om de zoveel tijd video frames van de buffer, decompresseert de frames, en laat deze op het scherm zien.)

\noindent Het transport van bestanden over TCP kan aanzienlijk veranderlijk zijn door het TCP congestie controle mechanisme. Pakketten kunnen vertraagd worden door TCP's herzendings mechanisme. Door deze kenmerken kan video streaming nooit goed werken over TCP.
Het gebruik van HTTP over TCP laat ook de video toe om gemakkelijker door firewalls en NAT's te gaan. Over HTTP streamen voorkomt de nood om een media controle server te hebben. Door deze voordelen gebruiken de meeste video streaming applicatie HTTP streaming (over TCP) als zijn onderliggende streaming protocol.

\subsubsubsection{Prefetching video}

\noindent Client buffering wordt gebruikt om de effecten van end-to-end vertragingen en verschillende beschikbare bandbreedtes te verzachten. Om bewaarde video te streamen, kan de client proberen om de video aan een hogere snelheid te downloaden dan de verbruikssnelheid, daardoor prefetching (op voorhand afhalen) van video frames zodat deze in de toekomst geconsumeerd kunnen worden. Deze prefetched video is natuurlijk bewaart in de applicatie buffer van de client. Zo’n prefetching komt voor bij TCP streaming, sinds TCP’s congestie vermijdings mechanisme zal proberen om alle beschikbare bandbreedte tussen server en client te gebruiken.



\subsubsubsection{Client applicatie buffer en TCP buffers}

\noindent Aan de server kant worden de bytes in de TCP send buffer gestopt. Als de TCP send buffer vol is, kan de server op dat moment geen bytes van de video file sturen.
Aan de client kant, leest de applicatie de bytes van de TCP receive buffer en plaatst deze bytes in de applicatie buffer. Op het zelfde moment pakt de applicatie om de zoveel tijd video frames van de applicatie buffer.

\noindent Stel de client pauzeert de video doorheen het streaming proces. Doorheen die periode worden er geen bytes weg gedaan van de applicatie buffer, maar er komen nog steeds bits de buffer binnen van de server. Als de buffer eindig is, zal het uiteindelijk vol geraken wat dan “back pressure” veroorzaakt tot helemaal terug naar de server. (Als TCP receive buffer van client vol is, kunnen er geen bytes uit de servers TCP send buffer want dan er voor zorgt dat server geen bytes meer kan versturen. In dit geval zal de server geblokkeerd worden totdat de gebruiker de video terug afspeelt.
Dus de server zend snelheid mag niet hoger zijn dan de video consumptie snelheid bij de client. Daardoor vormt een volle client applicatie buffer een limiet op de snelheid dat een video gestuurd kan worden van server naar client bij het streamen over HTTP.

\subsubsubsection{Analyse van video streaming}

\noindent Laten we veronderstellen dat de server bits tegen een constante snelheid x stuurt wanneer de client buffer niet vol is. Veronderstel op tijd t= 0, is de applicatie buffer leeg en begint de video binnen te komen in de client buffer. Wat is de tijd t = tp wanneer het afspelen begint? Wat is de tijd t = tf dat de client buffer vol gaat zijn?

\noindent Laten we eerst tp bepalen. De tijd wanneer Q bits in de applicatie buffer zitten begint het afspelen. Dus de tijd die nodig is om Q bits op te bouwen is tp = Q/x.
Laten we nu tf bepalen. We observeren eerst dat als x < r (server zend snelheid is lager dan de afspeelsnelheid), de buffer nooit vol zal geraken. Dus als de beschikbare snelheid in het netwerk lager is dan de video snelheid, zal de afspeling afwisselen tussen periodes van continue afspeling en bevriezing.

\noindent Laten we nu tf bepalen bij x > r. In dit geval, startend van tp, zal de buffer toenemen van Q tot B aan de snelheid van x – r sinds de bits verminderen aan de snelheid r en binnenkomen aan de snelheid x. Dus als de beschikbare snelheid in een netwerk hoger is dan de video snelheid zal de gebruiker, na de initiatie buffer vertraging, kunnen genieten van continue afspeling.



\subsubsubsection{Vroege beëindiging en herpositionering van de video}

\noindent HTTP streaming systemen maken van gebruik van de HTTP byte-range header in de HTTP GET request, wat dan precies de radius van de bytes specificeerd dat de client wilt krijgen van de gewenste video. Dit is handig wanneer de gebruiker wil springen naar verder punt in tijd van de video. Wanneer de client de positie herzet, zal de client een nieuwe HTTP request sturen, waar aangegeven in de byte radius header van welke byte in de file de server de data moet zenden. Wanneer de server een nieuwe HTTP request krijgt, kan het voorgaande request vergeten en in plaats bytes versturen beginnend met de byte die aangeduid is in de byte-range request.

\noindent Als de gebruiker zich naar een verder punt in de video verplaatst of de video eerder beëindigd, gaat prefetched maar nog niet geziene data verzonden door de server verloren, en dus een verspilling van netwerk bandbreedte en server resources. Om de kost te minimaliseren, gaan streaming systemen enkel een gemiddelde grootte buffer gebruiken om zo de hoeveelheid van prefetched video te limiteren door gebruik te maken van de byte-range header in de HTTP requests.



\subsubsection{Adaptive streaming en DASH}

\noindent HTTP streaming heeft een belangrijke tekortkoming: alle clienten krijgen dezelfde encodering van de video, ondanks de grote variaties in de hoeveelheid bandbreedte die beschikbaar is voor de client, zowel voor verschillende clienten en ook over tijd bij dezelfde client. Dit heeft geleidt naar de ontwikkeling van een nieuwe HTTP gebaseerde streaming, meestal gerefereerd naar \textbf{\acrfull{dash}}. In DASH wordt de video in verschillende  versies gecodeerd, met elke versie die een andere bit ratio en bijhorend een verschillend kwaliteitsniveau. De client vraagt dynamisch naar delen van video segmenten van een paar seconden. Als de hoeveelheid bandbreedte hoog is, zal de client stukken van een hogere ratio versie selecteren. Bij een lage bandbreedte zal het een lage ratio versie selecteren.
Langs één kant laat DASH clienten met verschillende internet toegangs ratio’s toe om video’s te streamen op een verschillend coderingsratio. Aan de andere kant laat DASH clienten toe om zich aan te passen aan de beschikbare bandbreedte. Deze functie is zeker belangrijk voor mobiele gebruikers.

\noindent Met DASH wordt elke video versie opgeslagen in de HTTP server met elk een verschillende URL. De HTTP server heeft ook een manifest file, die voorziet een URL voor elke versie samen met zijn bit ratio. De client vraagt eerst de manifest file en komt de verschillende versies te weten. De client kiest telkens een stuk per keer door telkens een URL en byte range in een HTTP GET request bericht te specifiëren. Bij het downloaden van de stukjes, berekent de client de gekregen bandbreedte en voert een rate determination algoritme uit zo het volgende stukje te vragen.

\noindent Door het dynamisch monitoren van de beschikbare bandbreedte en client buffer niveau en het doorstuur snelheid aan te passen met de versie wisseling, kan DASH vaak zorgen voor continue afspeling aan de best mogelijke kwaliteit zonder dat het beeld bevriest of overslaat. Bovendien, sinds dat de client de informatie onderhoud om te bepalen welk stuk er vervolgens gestuurd moet worden, verbeterd dit de schaalbaarheid van de server-side. Een ander voordeel van deze benadering is dat de client de HTTP byte-range request kan gebruiken om precies de hoeveelheid van opgehaalde video die in de locale buffer zit te controleren.



\subsubsection{Content Distribution Networks}

\noindent Voor een video internet bedrijf, is misschien de meest voor de hand liggende benadering om alle streaming video diensten te voorzien door een enkele massieve data center te maken, om al zijn video’s in op te slaan, en vandaar de video’s direct te streamen van de data center naar de clienten. Maar er zijn drie problemen bij deze benadering.

\noindent Ten \textbf{eerste}, als de client ver weg is van de data  center, gaan de pakketten vele communicatie linken kruisen, en waarschijnlijk door veel ISPs gaan, met de mogelijkheid dat de ISPs gevestigd zijn op een ander continent. Als één van die linken een doorvoer heeft die lager is dan consumptie snelheid, zal dit resulteren in vervelende bevriezingen.

\noindent Een \textbf{tweede} nadeel is dat een populaire video waarschijnlijk vaak over dezelfde communicatie linken gestuurd zal worden. Dit verspilt niet enkel bandbreedte, maar de video bedrijf zelf zal zijn ISP provider moeten betalen voor het zenden van telkens dezelfde bytes.

Een \textbf{derde} probleem is dat een enkele data center een enkel punt van falen representeert, als de data center of zijn linken naar het internet plat gaan, kan het geen enkele video meer verdelen.

\noindent Met het oog op de uitdaging om grote hoeveelheden video data te distribueren naar gebruikers die verspreidt zijn over de wereld, maken alle belangrijke video-streaming bedrijven gebruik van \textbf{\acrfull{cdn}}. 

\noindent Een CDN beheert server in meerdere geografisch verspreide locaties, bewaart kopieën van de video’s (en andere types zoals documenten, afbeeldingen, geluid) in zijn servers, en probeert om elke gebruikers aanvraag naar een CDN locatie te leiden die de beste gebruikers ervaring zal aanbieden. De CDN kan een privé CDN zijn, die is in bezit door de inhoud voorziener zelf. De CDN kan ook een third-party CDN zijn die inhoud verspreid namens meerdere inhoud voorzieners.

\noindent CDNs nemen meestal één van de twee andere server plaatsings filosofieën:

\begin{itemize}
   \item \textbf{Enter deep}. Een filosofie is om diep in te voeren in de toegangs netwerken van de ISPs (Internet Service Providers), door cluster servers op te zetten in toegangs ISPs over heel de wereld. De bedoeling is om zo dicht mogelijk bij de eind gebruiker te komen, door de gebruiker waargenomen vertraging en doorvoor te verbeteren door de aantal van linken en routers tussen de eind gebruiker en CDN cluster te verminderen vanwaar het de inhoud krijgt. Als gevolg van dit sterk gedistribueerd ontwerp, wordt de taak van het onderhouden en beheer van de cluster uitdagend.
 \item \textbf{Bring home}. Een tweede filosofie is om de ISPs thuis te brengen door grote clusters op een kleiner aantal sleutel locaties te bouwen en deze clusters connecteren door gebruik te maken van hoge snelheid prive netwerk. In plaats van binnen de toegangs ISPs te geraken, gaan deze CDNs  elke cluster op een locatie plaatsen die tegelijk dicht bij de PoPs van veel tier-1 ISPs ligt.
\end{itemize}

\noindent Bring home vergelijkend met Enter deep is dat de bring home resulteert in een lagere onderhoud en beheer overhead, wat eventueel ten koste gaat aan hogere vertraging en een lagere doorvoer naar eind gebruikers.

\noindent Eenmaal dat de clusters op zijn plaats staan, gaat de CDN zijn inhoud kopiëren over zijn clusters. De CDN wilt niet een kopie van elke video in elke cluster plaatsen, sinds sommige video’s amper bekeken worden en anderen enkel populair zijn sommige landen. In feite, veel CDNs pushen hun video’s niet naar hun clusters, maar gaan in de plaats een simpele pull strategie. Als een client een video vraagt aan een cluster die de video niet bewaart, dan gaat de cluster die video ophalen en bewaart een kopie en streamed terwijl de video naar de client. Gelijklopend met internet caches, wanneer de opslag ruimte vol geraakt, zal het de video’s verwijderen die het minst gevraagd worden.

\subsubsubsection{CDN Operation}

Wanneer  een browser in een gebruikers host wordt opgedragen om een specifieke video op te halen, moet de CDN het verzoek onderscheppen zodat een geschikte CDN server cluster voor die client kan bepalen op dat moment, en leidt de clients aanvraag om naar een server in die cluster.
Meeste CDNs nemen voordeel uit de DNS om aanvragen te onderscheppen en omleiden.



\subsubsubsection{Cluster selectie strategieën}

In de kern van elke CDN implementatie is een cluster selection strategy, dat is een mechanisme om clienten dynamisch te leiden naar een server cluster of een data center binnenin de CDN. De CDN kan op basis van het IP adres van de client een gepaste cluster selecteren.
Hierop volgend een overzicht van benaderingen, elk met zijn eigen voor en nadelen.

\noindent Een simpele strategie om een client aan een cluster toe te wijzen die geografisch het dichtstbijzijnd is. Door gebruik te maken van commerciële geo-locatie databanken, wordt elk LDNS IP adres gemapped bij een geografische locatie. Wanneer een DNS vraag ontvangen is van een bepaalde LDNS, kiest de CDN de cluster die het dichtstbij is. Dus de cluster die het minste aantal kilometers verwijderd is van de LDNS in vogelvlucht.
Voor sommige clienten kan dit een probleem vormen, omdat de dichts bijzijnde geografische cluster niet de dichts bijzijnde cluster langs het netwerk pad is. Daarnaast is er nog een probleem met alle DNS gebaseerde benaderingen is dat sommige eind gebruikers geconfigureerd zijn om remote gelokaliseerde LDNS te gebruiken, in welk geval de LDNS locatie ver weg kan zijn van de client zijn locatie. Deze strategie negeert ook de variatie in vertraging en beschikbare bandbreedte.

\noindent Om de beste cluster voor een client te bepalen op basis van het huidige verkeers voorwaarden, kan CDN in plaats een real-time measurements uitvoeren van vertraging en verlies prestaties tussen hun clusters en clienten. Een CDN kan elk van zijn clusters om de zoveel tijd probes laten sturen naar alle LDNSs in de wereld. Een nadeel is dat veel LDNSs zo geconfigureerd zijn om niet te antwoorden op zo’n probes.

\noindent Een alternatief voor het zenden van vreemd verkeer voor het meten van pad kenmerken is om de kenmerken van recent verkeer tussen clienten en CDN servers te gebruiken. De vertraging tussen een client en een cluster kan geschat worden door het verschil te onderzoeken tussen een server-to-client SYNACK en een client-to-server ACK tijdens de TCP three-way handshake. Zo’n oplossing vereist om clienten om te leiden naar sub-optimale clusters van tijd tot tijd om zo de kenmerken van de paden naar deze clusters te berekenen. Ook al zijn er maar een klein aantal requests nodig die als probe dienen, de geselecteerde clienten kunnen leiden van een significante prestatie vermindering.
Een ander alternatief voor cluster-to-client pad probing is om DNS query verkeer te gebruiken om de vertraging tussen clienten en clusters te berekenen.

\noindent Een heel andere benadering om clienten te matchen met CDN servers is om IP anycast te gebruiken. Het idee achter IP anycast is om de routers in de internet route pakketten van de client naar de “dichtstbijzijnde” cluster te hebben, als bepaald door de \textbf{\acrfull{bgp}}. Doorheen de IP-anycast configuratie fase, wijst de CDN bedrijf hetzelfde IP adres toe aan elk van zijn clusters en gebruikt de standaard BGP om dit IP adres te adverteren van elk van de verschillende cluster locaties. Wanneer een BGP router meerdere route advertenties krijgt voor ditzelfde IP adres, behandeld het deze advertenties als het voorzien van verschillende paden naar dezelfde fysieke locatie. Na deze initiële configuratie fase, kan de CDN zijn werk van inhoud te verspreiden doen. Wanneer een client een video wilt zien, geeft de CDN’s DNS de anycast adres terug, ongeacht de locatie van de client. Als de client een pakket naar dat IP adres stuurt, wordt het pakket gerouted naar de “dichtstbijzijnde” cluster als bepaald door de voorgedefinieerde doorstuur tabellen.
Deze benadering heeft het voordeel van clusters te vinden die het dichts bij de client is in plaats van de cluster die het dichts bij de client LDNS is. Maar de IP anycast houd geen rekening met het dynamische internet.



\subsubsection{Case studie: Youtube en Kankan}

\subsubsubsection{Youtube}

Youtube maakt uitgebreid gerbuik van CDN technologie om zijn video’s te verspreiden. Google gebruikt zijn eigen private CDN om video’s te verspreiden. Google heeft server clusters geïnstalleerd op meer dan 100 verschillende locaties. Van een subnet van ongeveer 50 van deze locaties, verdeeld Google YouTube video’s. Google gebruikt DNS om een client request om te leiden naar een specifieke cluster. De strategie die heet meeste gebruikt wordt is dat de client opgeleid wordt naar de cluster waar de RTT (Round Trip Time) tussen client en cluster het laagste is. Om de belasting te verdelen over de clusters, wordt de client soms omgeleid (via DNS) naar een verdere cluster. Daarnaast als een cluster de gevraagde video niet heeft, gaat de cluster in plaats van het ergens te halen, kan de cluster een HTTP omleid bericht sturen, om de client naar een andere cluster om te leiden. YouTube gebruikt HTTP streaming. Youtube maakt vaak een klein aantal verschillende beschikbare versies voor een video, elk met een andere bit rate en overeenkomstige qualiteits niveau. Sinds 2011, gebruikt het geen adaptive streaming (zoals DASH) meer, maar in plaats daarvan vereist de gebruiker manueel een versie te selecteren. Om bandbreedte en server resources te sparen die verspild zouden zijn door herplaatsing of vroege beïndiging, gebruikt YouTube de HTTP byte range request om de stroom van verzonden data te limiteren nadat een bepaalde hoeveelheid van de video vooraf is afgehaald.

\subsubsubsection{Kankan}

Op een hoog niveau, is P2P video streaming zeer gelijkend aan BitTorrent bestand downloaden. Wanneer een peer een video wilt zien, contacteerd het de tracker (welke gecentraliseerd kan zijn of peer-based door een DHT (Distributed Hash Table) te gebruiken) om andere peers te ontdekken in het systeem die een kopie hebben van die video. Deze peer vraagt vervolgens stukken van het video bestand in parallel van deze andere peers die het bestand hebben. Anders dan dowloaden met BitTorrent, echter zijn vragen bij voorkeur gemaakt voor delen die herspeelt gaan worden in de nabije toekomst om zo continue afspeling te verzekeren. De Kankan ontwerp gebruikt een tracker en zijn eigen DHT om inhoud op te sporen. Zwerm maten voor de meest populair inhoud, bevat meer dan 10 000 peers. De Kankan protocolen, voor communicatie tussen peer en tracker, tussen peer en DHT en tussen peers, zijn allemaal gepatenteerd. Om video stukjes te verspreiden tussen peers, gebruikt Kankan wanneer het mogelijk is UDP.
